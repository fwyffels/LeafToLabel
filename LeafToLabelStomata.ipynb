{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color: #690027;' markdown=\"1\">\n",
    "    <h1>From leaf to label: stomata detection</h1> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to upload and test your own photomicrographs with the deep learning system discussed in the paper *From leaf to label: a robust automated workflow for stomata detection* by *Sofie Meeus, Jan Van den Bulcke and Francis wyffels*.\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1g1wIt37A07yDi7w9uCza3eMt1oEPLJ5v\" alt=\"Overview\" style=\"width:600px;\"/>\n",
    "As illustrated above, your photomicrograph (A) will be divided in small overlapping patches (B) by using a sliding window approach. A deep neural network (VGG19) is trained to label these patches (C). Positively labeled patches of a photomicrograph (D) are clustered which results in the detection (E) depending on the threshold of your choice.\n",
    "\n",
    "To start, please run the following cell by clicking the button \"run\" or by using shift-enter.\n",
    "\n",
    "Before starting, if you run this notebook on Colab you might want GPU acceleration. Therefore click: *Edit* > *Notebook Settings* > *Hardware accelerator GPU* > *Save*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "We start by loading a few python libraries:\n",
    "\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/): a handy Python imaging library\n",
    "- [numpy](https://numpy.org): the fundamental package for scientific computing with Python\n",
    "- [sklearn](https://scikit-learn.org/stable/): the scikit-learn machine learning package, more specifically the clustetring functionality\n",
    "- [os](https://docs.python.org/3/library/os.html): a Python library for using operating system dependent functionality, e.g., reading, writing, listing files\n",
    "- [matplotlib](https://matplotlib.org): a Python library for making graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to specify which NVidia GPU we will use we need to run the following commands. This assumes that Keras and Tensorflow are GPU enabled. See the [TensorFlow](https://www.tensorflow.org/install/gpu) documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is a NVidia enable GPU run this\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that produced the results presented in the manuscript is based on Tensorflow and Keras. At the time of the writing, TensorFlow 2.0 was not yet available. However, since we relied on a TF 1.0 and Keras implementation. However, the principles for training your own deep learning model stay the same and should be sufficient to get started with [Keras](https://keras.io/getting_started/intro_to_keras_for_researchers/) yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, Conv2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import get_file\n",
    "from keras.utils import layer_utils\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a deep learning model you need data. As discussed before, the deep learning model we discuss here will be trained based on rectangular patches. In order to get a robust model both, positive and negative examples must be presented to the system.\n",
    "\n",
    "The data needs to be split into three parts:\n",
    "- The training set, i.e., the data which is used for changing the weights of the (deep) neural network;\n",
    "- The validation, i.e., the data which is used to see how well the learning process goes and to tune the model's hyperparameters;\n",
    "- The test data, i.e., the data you feed to the trained system afterwards when the detector is deployed.\n",
    "\n",
    "This notebook concerns the training and validation of the deep learning system for stomata detection, and a small dataset limited to *Carapa procera* is used for didactic purposes. This also restrains the computational needs (a full training with multiple species (cf. the paper) needs more patience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading and unzipping the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/record/3902280/files/data.zip\n",
    "!unzip 'data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./data/training/\"\n",
    "val_dir = \"./data/validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation data consists of patches of 120 by 120 pixels. A positive patch shows a stomata:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=11oZG14b8ZnbzooeeQpYaQeUe8wSbErmV\"  width=\"120\" />\n",
    "    \n",
    "A negative patch of *Carapa procera* has no stomata (or only partially) within the patch:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vwcYiZDJffjQ6gXCpThwvbcCoPhgseBS\"  width=\"120\" />\n",
    "\n",
    "In order to obtain these patches you will need labeled microphotographs (i.e., microphotographs of which you have the x,y coordinates of the center position of the stomata that are shown). Based on these labels, the patches can be cropped by using the [crop function of PIL](https://pillow.readthedocs.io/en/stable/reference/Image.html) or simply by [matrix slicing](https://numpy.org/doc/1.18/reference/arrays.indexing.html) in Python Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessor defines the data augmentation that will be applied to the dataset. This consists of random rotations, horizontal and vertical flips of the patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=180, horizontal_flip=True, vertical_flip=True, rescale=1/255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the data augmentation that will be applied, we need to set a [ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) that defines the size of the patches (120 x 120 pixels), the color mode, the batch size (i.e., the number of samples used in one training iteration), the classification mode of the task (i.e., binary classification: a patch can be positive or negative), whether the data needs to be shuffled or not and the seed (starting point) of the random number generator. Additionally, you need to provide a path to the directory with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"./data/training/\",\n",
    "    target_size=(120, 120),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=53\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of validation you also need to provide a ImageDataGenerator. This is defined with the same properties as the one for training but without data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        r\"./data/validation/\",\n",
    "        target_size=(120, 120),\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from a [VGG19](https://arxiv.org/abs/1409.1556) with two dense layers on top. The convolutional neural layers are pre-trained on [ImageNet](https://ieeexplore.ieee.org/abstract/document/5206848). Consequently, only the dense layers are trained. These pre-trained weights can be downloaded from Keras by adding the keyword \"imagenet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dense_neurons = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 Fine tuned\n",
    "from keras.applications import VGG19\n",
    "# We start from a VGG19 base (convolutional neural layers) with weights pre-trained on ImageNet\n",
    "vgg19_base = VGG19(weights='imagenet',include_top=False,input_shape=(120,120,3))\n",
    "x = vgg19_base.output\n",
    "x = Flatten()(x)\n",
    "# We add our own dense layered classifier on top\n",
    "x = Dense(2*number_dense_neurons,activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(number_dense_neurons,activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Output layer\n",
    "x = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vgg19_base.input, outputs=x)\n",
    "\n",
    "# Only the dense layers are trained, hence, the convolutional neural layers are set not trainable:\n",
    "for layer in vgg19_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# How does the network looks like\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters were optimized by using the [Adam](https://arxiv.org/pdf/1412.6980.pdf) learning rule for which the learning rate was tuned and finally set to 0.000005. Aditionally, we need to configure the training loss and metrics for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.000005\n",
    "# Initiate Stochastic Gradient Descent with momentum, learning rate to tune\n",
    "opt = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# Define the losses and metrics for validation\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training can be done by using the function *fit*. We train for 50 epochs. Note that we configured our architecture in a way that only the weights of the dense layers are adjusted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready, our network is trained and can be deployed for use. In order to be able to use your system, you need to save the model parameters. This can be done by calling *model.save(path)* with *path*, the path to the file in which you want to save the parameters. Additionally, the *fit* returns a *history* object. This records the training and validation progress over the epochs. Consequently, this is useful to keep track of the training process, for example, when comparing multiple hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Carapa procera deep learning model\n",
    "model.save('my_carapa_procera_model')\n",
    "\n",
    "# Plot the training and validation losses over time\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a deep learning model\n",
    "\n",
    "Now we trained a first deep learning model on the *Carapa procera*. This is stored into the object *model*. If you want to start from a saved deep learning model you just load it from a file by calling: *model = load_model(path_to_model)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and detection parameters\n",
    "\n",
    "The model makes use of a sliding window approach. Although not the most (computational) efficient, it's very simple to understand. The window has a size of 120 by 120 pixels and we use a step of 10 pixels. We start by loading our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_image = './data/Carapa_procero_demo.jpg' # you can use any other Carapa procero microphotograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(demo_image)\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "image = np.array(image) # conversion to a Numpy array\n",
    "ax.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 10\n",
    "patch_size = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also part of the detection parameters is the number of slides we will do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_x_shifts = (np.shape(image)[0] - patch_size) // shift\n",
    "no_y_shifts = (np.shape(image)[1] - patch_size) // shift\n",
    "print(\"We will do \"+str(no_x_shifts*no_y_shifts)+\" slides. Consequently, the deep learning model will be applied to \"+str(no_x_shifts*no_y_shifts)+\" windows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with a deep learning model\n",
    "\n",
    "Now that we identified all the windows, we apply the deep learning model. This is done by calling the *predict* function. The image we apply needs to be converted and normalized before applying the deep learning model. Since the output of the deep learning model is between 0 and 1, we also have to set a threshold from which we accept the output as a positive classification. The higher this threshold, the more precise the system will be in detecting stomata. However, if the threshold is too high, the system won't be able to detect stomata at all. Here we use the same threshold as determined in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stomata = []\n",
    "offset = patch_size // 2\n",
    "for x in np.arange(no_x_shifts + 1):\n",
    "    for y in np.arange(no_y_shifts + 1):\n",
    "        # center of the window\n",
    "        x_c = x * shift + offset\n",
    "        y_c = y * shift + offset\n",
    "        \n",
    "        # extraction of the window and conversion before applying the deep learning model\n",
    "        patch = image[x_c - offset:x_c + offset, y_c - offset:y_c + offset, :]\n",
    "        patch = patch.astype('float32')\n",
    "        patch /= 255\n",
    "        \n",
    "        # applying the deep learning model\n",
    "        y_model = model.predict(np.expand_dims(patch, axis=0))\n",
    "\n",
    "        # if the output of the network is above the \n",
    "        if y_model[0] > threshold:\n",
    "            stomata.append([x_c, y_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the detected stomata\n",
    "\n",
    "All positively labeled patches are clustered by using mean shift clustering. This technique groups neighboring (or even overlapping) positively labeled patches from which the resulting stoma coordinates are derived. Therefore, we can rely on the package [MeanShift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html) which is available in [scikit-learn](https://scikit-learn.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = patch_size // 2\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(stomata)\n",
    "stomata = np.array([[x[1], x[0]] for x in ms.cluster_centers_]) # cluster_centers_ is inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.imshow(image)\n",
    "ax.plot(stomata[:,0], stomata[:,1], 'xr', alpha=0.75, markeredgewidth=3, markersize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
